{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 4: Web Scraping Job Postings\n",
    "\n",
    "## Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal has two main objectives:\n",
    "\n",
    "   1. Determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "   2. Determine the factors that distinguish job categories and titles from each other. For example, can required skills accurately predict job title?\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.*\n",
    "\n",
    "Hint: Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries. \n",
    "\n",
    "**Goal:** Scrape your own data from a job aggregation tool like Indeed.com in order to collect the data to best answer these two questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Test the scraping code to ensure that the correct fields are scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = 'https://www.indeed.com/jobs?q=data+scientist&l=New+York'\n",
    "#conducting a request of the stated URL above:\n",
    "page = requests.get(URL)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "#print(soup.prettify())   # inserted a comment to make the notebook more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data Scientist',\n",
       " u'Junior Data Scientist',\n",
       " u'Data Scientist',\n",
       " u'Data Scientist - New York',\n",
       " u'Data Scientist',\n",
       " u'Data Scientist (Product)',\n",
       " u'Data Scientist and Machine Learning Researcher',\n",
       " u'Data Scientist - Journey Analytics',\n",
       " u'Data Scientist - Marketing',\n",
       " u'Data Analyst / Data Scientist']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Job Title\n",
    "\n",
    "def extract_job_title_from_result(soup):\n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "            jobs.append(a['title'])\n",
    "    return(jobs)\n",
    "extract_job_title_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Groupon',\n",
       " u'DiMeo Schneider & Associates',\n",
       " u'NORC at the University of Chicago',\n",
       " u'American College of Surgeons',\n",
       " u'NORC at the University of Chicago',\n",
       " u'The University of Chicago',\n",
       " u'Bobit Business Media',\n",
       " u'Natural Resources Defense Council',\n",
       " u'Rush University Medical Center',\n",
       " u'Nielsen']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Company Name\n",
    "\n",
    "def extract_company_from_result(soup):\n",
    "    companies = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        company = div.find_all(name='span', attrs={'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "        for span in sec_try:\n",
    "            companies.append(span.text.strip())\n",
    "    return(companies)\n",
    " \n",
    "extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'New York, NY',\n",
       " u'New York, NY',\n",
       " u'New York, NY',\n",
       " u'New York, NY',\n",
       " u'New York, NY',\n",
       " u'New York, NY 10011 (Chelsea area)',\n",
       " u'New York, NY',\n",
       " u'New York, NY 10022 (Midtown area)',\n",
       " u'New York, NY',\n",
       " u'New York, NY']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Location\n",
    "\n",
    "def extract_location_from_result(soup):\n",
    "    locations =[]\n",
    "    spans=soup.find_all('span',attrs={'class':'location'})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    return(locations)\n",
    "\n",
    "extract_location_from_result(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"no-wrap\"><b>relevance</b> -\\n            <a href=\"/jobs?q=data+scientist&amp;l=New+York&amp;sort=date\" rel=\"nofollow\">date</a></span>,\n",
       " <span class=\"no-wrap\">\\n                $90,000 - $115,000 a year</span>,\n",
       " <span class=\"no-wrap\">\\n                $155,000 - $165,000 a year</span>,\n",
       " <span class=\"no-wrap\">\\n                $65 - $75 an hour</span>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the tags for the salary\n",
    "\n",
    "span=soup.find_all('span',attrs={'class':'no-wrap'})\n",
    "span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'We are looking for a creative and innovative data scientist to drive strategic initiatives that will propel the future growth of Groupon....',\n",
       " u'DiMeo Schneider & Associates, L.L.C., a Chicago investment management consulting firm, seeks an experienced and ambitious professional for the position of...',\n",
       " u'Data mining, data analytics, \"Big Data,\" administrative data linkage; Through our projects and presentations before Congress, federal, state and local...',\n",
       " u'Solid understanding of statistical reporting, data quality and data management programming principles (e.g., transposing and summarizing raw data and...',\n",
       " u'This position is responsible for performing moderately complex tasks related to research design, data collection and analysis, developing instrumentation for...',\n",
       " u'Experience with health and claims data highly appreciated. Ability to work discretely with sensitive and confidential data required....',\n",
       " u'Analyze and interpret various data sets including survey results, behavioral data, and 3rd-party sources. Bachelor\\u2019s degree preferred (ideally with research,...',\n",
       " u'The Policy Analyst will play a key role in acquiring and analyzing data on natural disaster risks, damages, and costs;...',\n",
       " u'Examine data to identify potential data inconsistencies. Knowledge or experience working with EHR data, big data, and cloud computing services is desired....',\n",
       " u'Data Scientist - 663. As a Data Scientist/Senior Data Scientist in this role, you will work collaboratively across various teams, both within and outside of...']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Job Summary\n",
    "\n",
    "def extract_summary_from_result(soup):\n",
    "    summaries=[]\n",
    "    spans = soup.findAll('span',attrs={'class':'summary'})\n",
    "    for span in spans:\n",
    "        summaries.append(span.text.strip())\n",
    "    return(summaries)\n",
    "\n",
    "extract_summary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the search parameters\n",
    "\n",
    "max_results_per_city = 200\n",
    "city_set = ['New+York','Chicago','San+Francisco','Austin','Seattle','Los+Angeles','Philadelphia','Atlanta','Dallas','Pittsburgh','Portland','Phoenix','Denver','Houston','Miami','Washington+DC','Boulder']\n",
    "columns = ['city','job_title','company_name','location','summary','salary']\n",
    "sample_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swm\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "# Scraping\n",
    "\n",
    "for city in city_set:\n",
    "    for start in range(0, max_results_per_city,10):\n",
    "        page=requests.get('http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=' + str(city) + '&start=' + str(start))\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(page.text, 'lxml', from_encoding='utf-8')\n",
    "        for div in soup.find_all(name='div',attrs={'class':'row'}):\n",
    "            num=(len(sample_df)+1) # row number for index of job posting in df\n",
    "            job_post=[]            # empty list for each posting\n",
    "            job_post.append(city)  # append city name\n",
    "            for a in div.find_all(name='a',attrs={'data-tn-element':'jobTitle'}):\n",
    "                job_post.append(a['title'])    # append job title\n",
    "            company = div.find_all(name='span',attrs={'class':'company'})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                    job_post.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name='span',attrs ={'class':'result-link-source'})\n",
    "                for span in c:\n",
    "                    job_post.append(span.text)\n",
    "            c = div.findAll('span',attrs={'class':'location'})\n",
    "            for span in c:\n",
    "                job_post.append(span.text)\n",
    "            d = div.findAll('span',attrs={'class':'summary'})  # summary text\n",
    "            for span in d:\n",
    "                job_post.append(span.text.strip())\n",
    "            try:                     # salary\n",
    "                div_two = div.find(name='span',attrs={'class':'no-wrap'})\n",
    "                job_post.append(div_two.text.strip())\n",
    "            except:\n",
    "                job_post.append('nothing_found')\n",
    "            sample_df.loc[num] = job_post\n",
    "                          \n",
    "sample_df.to_csv('[scrape2].csv',encoding='utf-8')\n",
    "\n",
    "                          \n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
